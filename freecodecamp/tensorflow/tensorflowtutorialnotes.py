# -*- coding: utf-8 -*-
"""TensorflowTutorialNotes1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sx68upga-g9vCjS1m0N6nbB8woEBcN0r

# Tensorflow Tutorial Notes

These are my notes while taking the [tensorflow 2 course on freecodecamp](https://youtu.be/tPYj3fFJGjk)

GPU OPTIONAL 

So here I downloaded tensorflow gpu to help me use tensorflow on gpu for faster compute times.
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install tensorflow
# %pip install tensorflow-gpu

"""# Start of coding bits

Firstly I am importing tensorflow
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x
import tensorflow as tf
import os

"""# Initalize

now I am initalizing the TPU and setting the device parameter. Before you start enable TPU in your runtime in google colabs.
"""

resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')
tf.config.experimental_connect_to_cluster(resolver)
# This is the TPU initialization code that has to be at the beginning.
tf.tpu.experimental.initialize_tpu_system(resolver)
print("All devices: ", tf.config.list_logical_devices('TPU'))

# set device as tpu 
tf.device('/TPU:0')

"""# Distribution Strategy

"Usually you run your model on multiple TPUs in a data-parallel way. To distribute your model on multiple TPUs (or other accelerators), TensorFlow offers several distribution strategies. You can replace your distribution strategy and the model will run on any given (TPU) device. Check the distribution strategy guide for more information." from [https://www.tensorflow.org/guide/tpu](https://www.tensorflow.org/guide/tpu) explains it well
"""

tfd = tf.distribute.TPUStrategy(resolver)

"""# Testing

let us test out the TPU with the tfd distribution strategy
"""

a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])
b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])

@tf.function
def matmul_fn():
  c = tf.matmul(a, b)
  return c

z = tfd.run(matmul_fn)

print(z)

"""# Tutorial Start

## Tensors

A tensor is a generalization of vectors and matrices to potentially higher dimensions. In tensorflow they represent n-dimensional arrays of base datatypes.

each tensor has a data type and shape

Data types:
- float32
- int32
- string
- etc.

shape: the dimension of data
"""

string = tf.Variable("This is a string", tf.string)
number = tf.Variable(324, tf.int16)
floating = tf.Variable(3.567, tf.float64)

"""## Rank and degree of Tensors

another word for rank is degree which simply means the number of dimensions involded in the tensor. the tensor above is of rank 0 which is scalar.

Below are higher degrees of tensors
"""

rank1_tensor = tf.Variable(["Rank 1"], tf.string)
rank2_tensor = tf.Variable([["test", "ok"], ["test", "yes"]], tf.string)

tf.rank(rank1_tensor)

tf.rank(rank2_tensor)

"""## Shape 

differnt from rank. It tells about the elements that exist in each dimension
"""

rank1_tensor.shape

rank2_tensor.shape

# here is a rank 3 tensor

rank3_tensor = tf.Variable([[1223, 1313, 1312], [4232, 3142, 2414], [413414, 423, 4342]], tf.int64)

# here is the shape

rank3_tensor.shape

"""## Change tensor shape

the number of elements of a tensor is the product of the sizes of all its shapes. There are often many shapes that have the same number of elements which are conivent to change
"""

# create tensor full of ones of shape [1, 2, 3] (1 interior list, and 2 lists inisde that interior list with 3 elements)
tensor1 = tf.ones([1, 2, 3])
print(tensor1)

# reshape the tensor with still 6 items but in a differnet shape.
# lets try [2, 3, 1] -> 2 interior lists with 3 lists inside each and 1 element each
tensor2 = tf.reshape(tensor1, [2, 3, 1])
print(tensor2)

# lets reshape tensor 1 again
# leets try [3, 2, 1] this tiem
tensor25 = tf.reshape(tensor1, [3, 2, 1])
print(tensor25)

# lets try another way to reshape it via -1 which tells it to
# it simply put auto calculates the other numbers based on the dimension given
# calc the size of dimension in of that value
tensor3 = tf.reshape(tensor2, [3, -1])
print(tensor3)

# or lets try to reeshape it to a differnt shape
tensor35 = tf.reshape(tensor1, [6, -1]) # 6 dimension
print(tensor35)

# or 1 dimension
tensor36 = tf.reshape(tensor1, [1, -1])
print(tensor36)

"""## Types of Tensor

- variable
- constant 
- place holder
- sparsetensor

apart from variable these are immutable

## Evaluating Tensor

to evalulate / get its value. To do this since tensors repsent a paritally complete comoputtation we will need to runa. asession to evaluate a tensor
"""

# repllace tensor with name of tensor
# with tf.Session() as sess:
#  tensor.eval()

"""# Now we are done with part 1 go on to part 2"""